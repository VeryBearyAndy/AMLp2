{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f0b007-32ed-40fa-b350-1dc8a961b4c1",
   "metadata": {},
   "source": [
    "<H1>AML Project 2: Decision Trees, SVMs and Ensemble Learning</H1>\n",
    "<p1>Andrew J Markland, ajm259@uakron.edu</p1> <br>\n",
    "<p1>The University of Akron: CEPS : School of Computer Science</p1><br>\n",
    "<p1>Applied Machine Learning: CPSC:436:010</p1><br>\n",
    "<p1>Dr. Zhong-Hui Duan </p1><br><br>\n",
    "<p1>Abstract: The purpose of this assingment is to display understanding and untilization of Decesion tree, ensemble learning</p1><br>\n",
    "<p1>and SVM's through predicting the likelyhood of strokes through models that we will train with data retrived from NHANES</p1><br><br>\n",
    "<p1>Disclaimer** While this project is public in my public repository on github, I do not discourage others from using it as a </p1><br>\n",
    "<p1>reference for their own implementations, but the work written here is expressly written and owned by Andrew J Markland.</p1><br>\n",
    "<p1>I do not condone nor tolerate academic dishonesty, and should the question of plagerism arise derived from my works</p1><br>\n",
    "<p1>copies of this repository shall be promptly submitted to the assigning professor and the department of student affairs.</p1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50852820-a890-4a82-a2da-bc20444a2261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T03:09:36.956125600Z",
     "start_time": "2024-04-02T03:09:34.962295900Z"
    }
   },
   "outputs": [],
   "source": [
    "#includes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Data Manager\n",
    "    This cell will handle partitioning and cleaning the data and provide a hand-full of useful\n",
    "    variables that I can call on to test different data sets based on a partition of the test data, this is primarily \n",
    "    going to be used to streamline the testing phases and make data balancing and acquistion faster. \n",
    "\"\"\"\n",
    "class DataWrapper:\n",
    "    def __init__(self):\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "\n",
    "class finalWrapper:\n",
    "    def __init__(self):\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        \n",
    "class testWrapper:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.ids = None\n",
    "\n",
    "initialDataPD = pd.read_csv(\"NHANES_data_stroke_train.csv\")\n",
    "test_DF = pd.read_csv(\"NHANES_data_stroke_test4Students.csv\")\n",
    "test_DF = test_DF.drop(\"stroke\", axis=1)\n",
    "\n",
    "def partitionAndBalance(dataObject):\n",
    "    global initialDataPD\n",
    "    X = initialDataPD.drop('stroke', axis=1)\n",
    "    y = initialDataPD['stroke']\n",
    "    dataObject.train_x, dataObject.test_x, dataObject.train_y, dataObject.test_y = train_test_split(X, y, test_size = 0.2)\n",
    "    balanceTrainData(dataObject)\n",
    "    imputeMissingTestValues(dataObject)\n",
    "    #print(dataObject.train_X)\n",
    "    return dataObject\n",
    "    \n",
    "    \n",
    "def balanceTrainData(dataObject):\n",
    "    combinedDataDF = pd.DataFrame(dataObject.train_x)\n",
    "    combinedDataDF['stroke'] = dataObject.train_y\n",
    "    \"\"\"partition based on classes\"\"\"\n",
    "    strokeDF = combinedDataDF[combinedDataDF['stroke'] == 1]\n",
    "    noStrokeDF = combinedDataDF[combinedDataDF['stroke'] == 2]\n",
    "    \"\"\"clean up missing values\"\"\"\n",
    "    noStrokeDF = noStrokeDF.dropna()\n",
    "    imputer = KNNImputer(n_neighbors=3)\n",
    "    imputedDF = imputer.fit_transform(strokeDF)\n",
    "    cleanStrokeDF = pd.DataFrame(imputedDF, columns=strokeDF.columns)\n",
    "    noStrokeDF = noStrokeDF.sample(n=len(cleanStrokeDF))\n",
    "    balancedDF = pd.concat([noStrokeDF, cleanStrokeDF])\n",
    "    dataObject.train_x = balancedDF.drop('stroke', axis=1)\n",
    "    dataObject.train_y = balancedDF['stroke']\n",
    "    \n",
    "def imputeMissingTestValues(dataObject):\n",
    "    imputer = KNNImputer(n_neighbors=3)\n",
    "    imputedTestDF = imputer.fit_transform(dataObject.test_x)\n",
    "    dataObject.test_x = pd.DataFrame(imputedTestDF, columns=dataObject.test_x.columns)\n",
    "    \n",
    "def dropRows(dataObject, rows):\n",
    "    for row in rows:\n",
    "        dataObject.test_x = dataObject.test_x.drop(row, axis=1)\n",
    "        dataObject.train_x = dataObject.train_x.drop(row, axis=1)\n",
    "        \n",
    "def scaleData(dataObject):\n",
    "    scaler = StandardScaler()\n",
    "    trainScaled = scaler.fit_transform(dataObject.train_x)\n",
    "    testScaled = scaler.transform(dataObject.test_x)\n",
    "    dataObject.test_x = pd.DataFrame(testScaled, columns=dataObject.test_x.columns)\n",
    "    dataObject.train_x = pd.DataFrame(trainScaled, columns=dataObject.train_x.columns)\n",
    "\n",
    "def imputeMissingValuesDataFrame(data):\n",
    "    imputer = KNNImputer(n_neighbors = 3)\n",
    "    cleanData = imputer.fit_transform(data)\n",
    "    data = pd.DataFrame(cleanData, columns=data.columns)\n",
    "    return data\n",
    "\n",
    "def dropRowsDataframe(data, rows):\n",
    "    for row in rows:\n",
    "        data = data.drop(row, axis=1)\n",
    "    return data\n",
    "\n",
    "def cutParticipantID(data):\n",
    "    patientIds = pd.DataFrame(data[\"ParticipantID\"], columns=[\"ParticipantID\"])\n",
    "    data.drop(\"ParticipantID\", axis=1);\n",
    "    return patientIds\n",
    "    \n",
    "def balanceFinalWrapper(Fwrapper):\n",
    "    #load data and separate\n",
    "    strokeDF = initialDataPD[initialDataPD['stroke'] == 1]\n",
    "    noStrokeDF = initialDataPD[initialDataPD['stroke'] == 2]\n",
    "    #clean the data\n",
    "    noStrokeDF = noStrokeDF.dropna()\n",
    "    imputer = KNNImputer(n_neighbors = 3)\n",
    "    cleanStroke = imputer.fit_transform(strokeDF)\n",
    "    strokeDF = pd.DataFrame(cleanStroke, columns=strokeDF.columns)\n",
    "    noStrokeDF = noStrokeDF.sample(n=len(strokeDF))\n",
    "    #join the data back together\n",
    "    FTrainDF = pd.concat([strokeDF, noStrokeDF])\n",
    "    #set the training data \n",
    "    Fwrapper.train_x = FTrainDF.drop(\"stroke\", axis=1)\n",
    "    Fwrapper.train_x = Fwrapper.train_x.drop(\"ParticipantID\", axis=1)\n",
    "    Fwrapper.train_y = FTrainDF[\"stroke\"]\n",
    "    return Fwrapper\n",
    "    \n",
    "def formatTestWrapper(Twrapper):\n",
    "    dirtyData = test_DF.drop(\"ParticipantID\", axis=1)\n",
    "    imputer = KNNImputer(n_neighbors = 3)\n",
    "    cleanData = imputer.fit_transform(dirtyData)\n",
    "    Twrapper.data = pd.DataFrame(cleanData, columns=dirtyData.columns)\n",
    "    Twrapper.ids = pd.DataFrame(test_DF[\"ParticipantID\"], columns=[\"ParticipantID\"])\n",
    "    return Twrapper\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T01:12:09.419165400Z",
     "start_time": "2024-04-03T01:12:09.380888700Z"
    }
   },
   "id": "12c64942179275f6"
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6239316239316239\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Random Forest run,\n",
    "    This cell is the testing of the random forest model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#getting a random distribution of data and balancing it\n",
    "rf_dataObject = DataWrapper()\n",
    "rf_dataObject = partitionAndBalance(rf_dataObject)\n",
    "\n",
    "noise = [\"Income\", \"Sex\", \"Race\", \"Edu\", \"Diabetes\", \"isInsured\"]\n",
    "dropRows(rf_dataObject, noise)\n",
    "\n",
    "# initializing the random forest\n",
    "randForest = RandomForestClassifier(n_estimators = 1000, max_depth = 12, min_samples_split=20, min_samples_leaf=15, max_features=5)\n",
    "\n",
    "randForest.fit(rf_dataObject.train_x, rf_dataObject.train_y)\n",
    "\n",
    "prediction = randForest.predict(rf_dataObject.test_x)\n",
    "accuracy = accuracy_score(rf_dataObject.test_y, prediction)\n",
    "print(accuracy)\n",
    "\n",
    "def getForestRun(accuracyList):\n",
    "    #getting a random distribution of data and balancing it \n",
    "    rf_dataObject = DataWrapper()\n",
    "    rf_dataObject = partitionAndBalance(rf_dataObject)\n",
    "    \n",
    "    #noise = [\"Income\", \"Sex\", \"Race\", \"Edu\", \"Diabetes\", \"isInsured\"]\n",
    "    #dropRows(rf_dataObject, noise)\n",
    "    rf_dataObject.train_x.drop(\"ParticipantID\", axis=1)\n",
    "    rf_dataObject.test_x.drop(\"ParticipantID\", axis=1)\n",
    "\n",
    "    # initializing the random forest\n",
    "    randForest = RandomForestClassifier(n_estimators = 1000, max_depth = 12, min_samples_split=20, min_samples_leaf=15, max_features=5)\n",
    "\n",
    "    randForest.fit(rf_dataObject.train_x, rf_dataObject.train_y)\n",
    "\n",
    "    prediction = randForest.predict(rf_dataObject.test_x)\n",
    "    accuracy = accuracy_score(rf_dataObject.test_y, prediction)\n",
    "    accuracyList.append(accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T01:04:38.501679100Z",
     "start_time": "2024-04-03T01:04:37.022149200Z"
    }
   },
   "id": "67b519ef41c9fb52"
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6532356532356532\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Gradient Boosted Tree\n",
    "    this cell is for testing the gradient boosted tree model\n",
    "\"\"\"\n",
    "\n",
    "#getting a Random Distribution of data and balancing it\n",
    "grad_dataObject = partitionAndBalance(DataWrapper())\n",
    "\n",
    "noise = [\"Income\", \"Sex\", \"Race\", \"Edu\", \"Diabetes\", \"BMI\"]\n",
    "dropRows(grad_dataObject, noise)\n",
    "\n",
    "grad_dataObject.test_x.drop(\"ParticipantID\", axis=1)\n",
    "grad_dataObject.train_x.drop(\"ParticipantID\", axis=1)\n",
    "\n",
    "gradTree = GradientBoostingClassifier(n_estimators=500, max_depth=10, min_samples_split=20, learning_rate=0.1, min_samples_leaf=15, max_features=5)\n",
    "gradTree.fit(grad_dataObject.train_x, grad_dataObject.train_y)\n",
    "\n",
    "prediction = gradTree.predict(grad_dataObject.test_x)\n",
    "accuracy = accuracy_score(grad_dataObject.test_y, prediction)\n",
    "print(accuracy)\n",
    "\n",
    "def getGradRun(accuracyList):\n",
    "    #getting a Random Distribution of data and balancing it\n",
    "    grad_dataObject = partitionAndBalance(DataWrapper())\n",
    "    #noise = [\"Income\", \"Sex\", \"Race\", \"Edu\", \"Diabetes\"]\n",
    "    dropRows(grad_dataObject, noise)\n",
    "    grad_dataObject.test_x.drop(\"ParticipantID\", axis=1)\n",
    "    grad_dataObject.train_x.drop(\"ParticipantID\", axis=1)\n",
    "\n",
    "    gradTree = GradientBoostingClassifier(n_estimators=500, max_depth=10, min_samples_split=20, learning_rate=0.1, min_samples_leaf=15, max_features=5)\n",
    "    gradTree.fit(grad_dataObject.train_x, grad_dataObject.train_y)\n",
    "\n",
    "    prediction = gradTree.predict(grad_dataObject.test_x)\n",
    "    accuracy = accuracy_score(grad_dataObject.test_y, prediction)\n",
    "    accuracyList.append(accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T23:45:04.016046400Z",
     "start_time": "2024-04-02T23:45:03.454910700Z"
    }
   },
   "id": "f8766bbefae8f682"
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6874236874236874\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Support Vector Machine\n",
    "    This cell is for testing the support Vector Machine model\n",
    "\"\"\"\n",
    "\n",
    "svm_data = partitionAndBalance(DataWrapper())\n",
    "SVM = SVC(kernel = 'rbf', C=.6, gamma=0.001)\n",
    "scaleData(svm_data)\n",
    "SVM.fit(svm_data.train_x, svm_data.train_y)\n",
    "\n",
    "prediction = SVM.predict(svm_data.test_x)\n",
    "\n",
    "accuracy = accuracy_score(svm_data.test_y, prediction)\n",
    "print(accuracy)\n",
    "\n",
    "def getSVMrun(accuracyList):\n",
    "    svm_data = partitionAndBalance(DataWrapper())\n",
    "    SVM = SVC(kernel = 'rbf', C=1, gamma=(1/18))\n",
    "    svm_data.train_x.drop(\"ParticipantID\", axis=1)\n",
    "    svm_data.test_x.drop(\"ParticipantID\", axis=1)\n",
    "    scaleData(svm_data)\n",
    "    SVM.fit(svm_data.train_x, svm_data.train_y)\n",
    "\n",
    "    prediction = SVM.predict(svm_data.test_x)\n",
    "\n",
    "    accuracy.append(accuracy_score(svm_data.test_y, prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T23:14:34.500117100Z",
     "start_time": "2024-04-02T23:14:34.389090100Z"
    }
   },
   "id": "58bbf0bfb012c2c7"
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6837606837606834\n",
      "0.5934065934065934\n",
      "0.757020757020757\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing cell\n",
    "  this cell is mostly just to run 100 runs of each model with random partitions of\n",
    "  the test data to see average performance across the runs\n",
    "\"\"\"\n",
    "\n",
    "accuracy = []\n",
    "for x in range(100):\n",
    "    getForestRun(accuracy)\n",
    "\n",
    "print(sum(accuracy)/len(accuracy))\n",
    "print(min(accuracy))\n",
    "print(max(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T01:07:08.707489100Z",
     "start_time": "2024-04-03T01:04:43.055284400Z"
    }
   },
   "id": "f69e472809b35dff"
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Random Forests Final Run\n",
    "    ***Discrete Values***\n",
    "\"\"\"\n",
    "#set up training data\n",
    "rfTrainData = balanceFinalWrapper(finalWrapper())\n",
    "#set up test data\n",
    "randTest = formatTestWrapper(testWrapper())\n",
    "#declare the model\n",
    "fRandForestDiscreet = RandomForestClassifier(n_estimators = 1000, max_depth = 12, min_samples_split=20, min_samples_leaf=15, max_features=5)\n",
    "fRandForestDiscreet.fit(rfTrainData.train_x, rfTrainData.train_y)\n",
    "#makePrediction\n",
    "rfDiscreetPrediction = fRandForestDiscreet.predict(randTest.data)\n",
    "#build output dataframe\n",
    "rfDiscreetPrediction = pd.DataFrame(rfDiscreetPrediction, columns=[\"stroke\"])\n",
    "#replace 2 with 0\n",
    "rfDiscreetPrediction = rfDiscreetPrediction[\"stroke\"].replace(2, 0)\n",
    "rfDiscreetPrediction = pd.concat([randTest.ids, rfDiscreetPrediction], axis=1, ignore_index=True, sort=False,)\n",
    "rfDiscreetPrediction.columns = [\"ParticipantID\", \"stroke\"]\n",
    "rfDiscreetPrediction.to_csv(\"./ResultsWithDiscreetValues/randomforest_pred.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T01:52:45.477731700Z",
     "start_time": "2024-04-03T01:52:43.917116600Z"
    }
   },
   "id": "d93024ca0dc9d629"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1fa5d086a6a91d86"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
