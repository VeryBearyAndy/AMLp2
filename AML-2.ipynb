{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f0b007-32ed-40fa-b350-1dc8a961b4c1",
   "metadata": {},
   "source": [
    "<H1>AML Project 2: Decision Trees, SVMs and Ensemble Learning</H1>\n",
    "<p1>Andrew J Markland, ajm259@uakron.edu</p1> <br>\n",
    "<p1>The University of Akron: CEPS : School of Computer Science</p1><br>\n",
    "<p1>Applied Machine Learning: CPSC:436:010</p1><br>\n",
    "<p1>Dr. Zhong-Hui Duan </p1><br><br>\n",
    "<p1>Abstract: The purpose of this assingment is to display understanding and untilization of Decesion tree, ensemble learning</p1><br>\n",
    "<p1>and SVM's through predicting the likelyhood of strokes through models that we will train with data retrived from NHANES</p1><br><br>\n",
    "<p1>Disclaimer** While this project is public in my public repository on github, I do not discourage others from using it as a </p1><br>\n",
    "<p1>reference for their own implementations, but the work written here is expressly written and owned by Andrew J Markland.</p1><br>\n",
    "<p1>I do not condone nor tolerate academic dishonesty, and should the question of plagerism arise derived from my works</p1><br>\n",
    "<p1>copies of this repository shall be promptly submitted to the assigning professor and the department of student affairs.</p1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50852820-a890-4a82-a2da-bc20444a2261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T03:09:36.956125600Z",
     "start_time": "2024-04-02T03:09:34.962295900Z"
    }
   },
   "outputs": [],
   "source": [
    "#includes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2723629342.py, line 76)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[299], line 76\u001B[1;36m\u001B[0m\n\u001B[1;33m    \u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Data Manager\n",
    "    This cell will handle partitioning and cleaning the data and provide a hand-full of useful\n",
    "    variables that I can call on to test different data sets based on a partition of the test data, this is primarily \n",
    "    going to be used to streamline the testing phases and make data balancing and acquistion faster. \n",
    "\"\"\"\n",
    "class DataWrapper:\n",
    "    def __init__(self):\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "\n",
    "\n",
    "\n",
    "initialDataPD = pd.read_csv(\"NHANES_data_stroke_train.csv\")\n",
    "test_DF = pd.read_csv(\"NHANES_data_stroke_test4Students.csv\")\n",
    "test_DF = test_DF.drop(\"stroke\", axis=1)\n",
    "\n",
    "def partitionAndBalance(dataObject):\n",
    "    global initialDataPD\n",
    "    X = initialDataPD.drop('stroke', axis=1)\n",
    "    y = initialDataPD['stroke']\n",
    "    dataObject.train_x, dataObject.test_x, dataObject.train_y, dataObject.test_y = train_test_split(X, y, test_size = 0.2)\n",
    "    balanceTrainData(dataObject)\n",
    "    imputeMissingTestValues(dataObject)\n",
    "    #print(dataObject.train_X)\n",
    "    return dataObject\n",
    "    \n",
    "    \n",
    "def balanceTrainData(dataObject):\n",
    "    combinedDataDF = pd.DataFrame(dataObject.train_x)\n",
    "    combinedDataDF['stroke'] = dataObject.train_y\n",
    "    \"\"\"partition based on classes\"\"\"\n",
    "    strokeDF = combinedDataDF[combinedDataDF['stroke'] == 1]\n",
    "    noStrokeDF = combinedDataDF[combinedDataDF['stroke'] == 2]\n",
    "    \"\"\"clean up missing values\"\"\"\n",
    "    noStrokeDF = noStrokeDF.dropna()\n",
    "    imputer = KNNImputer(n_neighbors=3)\n",
    "    imputedDF = imputer.fit_transform(strokeDF)\n",
    "    cleanStrokeDF = pd.DataFrame(imputedDF, columns=strokeDF.columns)\n",
    "    noStrokeDF = noStrokeDF.sample(n=len(cleanStrokeDF))\n",
    "    balancedDF = pd.concat([noStrokeDF, cleanStrokeDF])\n",
    "    dataObject.train_x = balancedDF.drop('stroke', axis=1)\n",
    "    dataObject.train_y = balancedDF['stroke']\n",
    "    \n",
    "def imputeMissingTestValues(dataObject):\n",
    "    imputer = KNNImputer(n_neighbors=3)\n",
    "    imputedTestDF = imputer.fit_transform(dataObject.test_x)\n",
    "    dataObject.test_x = pd.DataFrame(imputedTestDF, columns=dataObject.test_x.columns)\n",
    "    \n",
    "def dropRows(dataObject, rows):\n",
    "    for row in rows:\n",
    "        dataObject.test_x.drop(row, axis=1)\n",
    "        dataObject.train_x.drop(row, axis=1)\n",
    "        \n",
    "def scaleData(dataObject):\n",
    "    scaler = StandardScaler()\n",
    "    trainScaled = scaler.fit_transform(dataObject.train_x)\n",
    "    testScaled = scaler.transform(dataObject.test_x)\n",
    "    dataObject.test_x = pd.DataFrame(testScaled, columns=dataObject.test_x.columns)\n",
    "    dataObject.train_x = pd.DataFrame(trainScaled, columns=dataObject.train_x.columns)\n",
    "\n",
    "def imputeMissingValuesDataFrame(data):\n",
    "    imputer = KNNImputer(n_neighbors = 3)\n",
    "    cleanData = imputer.fit_transform(data)\n",
    "    data = pd.DataFrame(cleanData, columns=data.columns)\n",
    "    return data\n",
    "\n",
    "def dropRowsDataframe(data, rows):\n",
    "    for row in rows:\n",
    "        data.drop(row, axis=1)\n",
    "    return data\n",
    "\n",
    "def stripIDs(data):\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T23:03:07.619904800Z",
     "start_time": "2024-04-02T23:03:07.603175200Z"
    }
   },
   "id": "12c64942179275f6"
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706959706959707\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Random Forest run,\n",
    "    This cell is the testing of the random forest model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#getting a random distribution of data and balancing it\n",
    "rf_dataObject = DataWrapper()\n",
    "rf_dataObject = partitionAndBalance(rf_dataObject)\n",
    "\n",
    "noise = [\"Income\", \"Sex\", \"Race\", \"Edu\", \"Diabetes\", \"isInsured\"]\n",
    "dropRows(rf_dataObject, noise)\n",
    "\n",
    "# initializing the random forest\n",
    "randForest = RandomForestClassifier(n_estimators = 1000, max_depth = 12, min_samples_split=20)\n",
    "\n",
    "randForest.fit(rf_dataObject.train_x, rf_dataObject.train_y)\n",
    "\n",
    "prediction = randForest.predict(rf_dataObject.test_x)\n",
    "accuracy = accuracy_score(rf_dataObject.test_y, prediction)\n",
    "print(accuracy)\n",
    "\n",
    "def getForestRun(accuracyList):\n",
    "    #getting a random distribution of data and balancing it \n",
    "    rf_dataObject = DataWrapper()\n",
    "    rf_dataObject = partitionAndBalance(rf_dataObject)\n",
    "\n",
    "    noise = [\"Income\", \"Sex\", \"Race\", \"Edu\", \"Diabetes\", \"isInsured\"]\n",
    "    dropRows(rf_dataObject, noise)\n",
    "\n",
    "    # initializing the random forest\n",
    "    randForest = RandomForestClassifier(n_estimators = 1000, max_depth = 12, min_samples_split=20)\n",
    "\n",
    "    randForest.fit(rf_dataObject.train_x, rf_dataObject.train_y)\n",
    "\n",
    "    prediction = randForest.predict(rf_dataObject.test_x)\n",
    "    accuracy = accuracy_score(rf_dataObject.test_y, prediction)\n",
    "    accuracyList.append(accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:17:53.329006300Z",
     "start_time": "2024-04-02T20:17:51.805752200Z"
    }
   },
   "id": "67b519ef41c9fb52"
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6288156288156288\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Gradient Boosted Tree\n",
    "    this cell is for testing the gradient boosted tree model\n",
    "\"\"\"\n",
    "\n",
    "#getting a Random Distribution of data and balancing it\n",
    "grad_dataObject = partitionAndBalance(DataWrapper())\n",
    "\n",
    "noise = [\"Income\", \"Sex\", \"Race\", \"Edu\", \"Diabetes\", \"BMI\"]\n",
    "dropRows(grad_dataObject, noise)\n",
    "\n",
    "gradTree = GradientBoostingClassifier(n_estimators=1000, max_depth=20, min_samples_split=20)\n",
    "gradTree.fit(grad_dataObject.train_x, grad_dataObject.train_y)\n",
    "\n",
    "prediction = gradTree.predict(grad_dataObject.test_x)\n",
    "accuracy = accuracy_score(grad_dataObject.test_y, prediction)\n",
    "print(accuracy)\n",
    "\n",
    "def getGradRun(accuracyList):\n",
    "    #getting a Random Distribution of data and balancing it\n",
    "    grad_dataObject = partitionAndBalance(DataWrapper())\n",
    "\n",
    "    noise = [\"Income\", \"Sex\", \"Race\", \"Edu\", \"Diabetes\"]\n",
    "    dropRows(grad_dataObject, noise)\n",
    "\n",
    "    gradTree = GradientBoostingClassifier(n_estimators=150, max_depth=10, min_samples_split=10)\n",
    "    gradTree.fit(grad_dataObject.train_x, grad_dataObject.train_y)\n",
    "\n",
    "    prediction = gradTree.predict(grad_dataObject.test_x)\n",
    "    accuracy = accuracy_score(grad_dataObject.test_y, prediction)\n",
    "    accuracyList.append(accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:26:42.497775800Z",
     "start_time": "2024-04-02T20:26:41.426202Z"
    }
   },
   "id": "f8766bbefae8f682"
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5494505494505495\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Support Vector Machine\n",
    "    This cell is for testing the support Vector Machine model\n",
    "\"\"\"\n",
    "\n",
    "svm_data = partitionAndBalance(DataWrapper())\n",
    "SVM = SVC(kernel = 'rbf', C=.6, gamma=0.001)\n",
    "scaleData(svm_data)\n",
    "SVM.fit(svm_data.train_x, svm_data.train_y)\n",
    "\n",
    "prediction = SVM.predict(svm_data.test_x)\n",
    "\n",
    "accuracy = accuracy_score(svm_data.test_y, prediction)\n",
    "print(accuracy)\n",
    "\n",
    "def getSVMrun(accuracyList):\n",
    "    svm_data = partitionAndBalance(DataWrapper())\n",
    "    SVM = SVC(kernel = 'rbf', C=1, gamma=(1/18))\n",
    "    scaleData(svm_data)\n",
    "    SVM.fit(svm_data.train_x, svm_data.train_y)\n",
    "\n",
    "    prediction = SVM.predict(svm_data.test_x)\n",
    "\n",
    "    accuracy.append(accuracy_score(svm_data.test_y, prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:56:48.623890200Z",
     "start_time": "2024-04-02T20:56:48.540378100Z"
    }
   },
   "id": "58bbf0bfb012c2c7"
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6725885225885225\n",
      "0.6141636141636142\n",
      "0.7765567765567766\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing cell\n",
    "  this cell is mostly just to run 100 runs of each model with random partitions of\n",
    "  the test data to see average performance across the runs\n",
    "\"\"\"\n",
    "\n",
    "accuracy = []\n",
    "for x in range(100):\n",
    "    getSVMrun(accuracy)\n",
    "\n",
    "print(sum(accuracy)/len(accuracy))\n",
    "print(min(accuracy))\n",
    "print(max(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:56:56.007897100Z",
     "start_time": "2024-04-02T20:56:50.264819700Z"
    }
   },
   "id": "f69e472809b35dff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T03:10:36.806847600Z",
     "start_time": "2024-04-02T03:10:36.801846700Z"
    }
   },
   "id": "d93024ca0dc9d629"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
